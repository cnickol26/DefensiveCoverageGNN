{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from multiprocessing import Process\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GlobalAvgPool, ECCConv\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "from spektral.utils import reorder\n",
    "from ipywidgets import Checkbox, Dropdown, Accordion, VBox\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from spektral.data import Dataset, Graph, DisjointLoader\n",
    "from spektral.layers import CrystalConv, GlobalAvgPool\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from os.path import isfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = pd.read_pickle('simple_nodes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = pd.read_pickle('adj_mat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feat = pd.read_pickle('edge_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feat = adj_mat * edge_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_pickle('y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reset_index()\n",
    "y['uniqueplayId'] = y['uniqueplayId'].astype(int)\n",
    "y = y.set_index(['uniqueplayId','frameId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = node_feat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat['uniqueplayId'] = node_feat['uniqueplayId'].astype(int)\n",
    "node_feat = node_feat.set_index(['uniqueplayId','frameId','nflId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_x                0\n",
       "new_y                0\n",
       "Defense              0\n",
       "frames_after_snap    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feat.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(index):\n",
    "    ## PLAY_ID MUST BE A STRING\n",
    "    \n",
    "    play_id = index[0]\n",
    "    frame_id = index[1]\n",
    "\n",
    "    # Node features\n",
    "    ## filter the node features matrix by given play id and frame id\n",
    "    x_temp = node_feat.query(f'(uniqueplayId=={play_id})&(frameId=={frame_id})')\n",
    "    x_temp = np.array(x_temp)\n",
    "    #print(x_temp.shape)\n",
    "\n",
    "    # Adjacency\n",
    "    a_temp = adj_mat[(play_id, frame_id)]\n",
    "    a_temp = sp.csr_matrix(a_temp)\n",
    "    #print(a_temp.shape)\n",
    "    \n",
    "    # Edges\n",
    "    ## get the correct edge matrix based on play id and frame id\n",
    "    e_temp = edge_feat[(play_id, frame_id)]\n",
    "    e_sp_mat = sp.find(e_temp)\n",
    "\n",
    "    edge_indeces = np.array([e_sp_mat[0], e_sp_mat[1]]).T\n",
    "\n",
    "    edge_vals = e_sp_mat[2]\n",
    "    e_temp = reorder(edge_indeces, edge_features=edge_vals)[1].reshape(len(edge_vals), 1)\n",
    "    #print(e_temp.shape)\n",
    "\n",
    "    # Labels\n",
    "    ## get the single label of coverage from y for that play id\n",
    "    y_temp = y.query(f'(uniqueplayId=={play_id})&(frameId=={frame_id})').values[0]\n",
    "    #print(y_temp.shape)\n",
    "\n",
    "    return Graph(x=x_temp, a=a_temp, e=e_temp, y=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=23, n_node_features=4, n_edge_features=1, n_labels=7)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_graph(('202109090097', 6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 54.9 s, total: 3min 35s\n",
      "Wall time: 37min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "# Load data\n",
    "################################################################################\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        all_graphs = []\n",
    "        indeces = adj_mat.index\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=40) as executor:\n",
    "            for r in executor.map(make_graph, indeces):\n",
    "                all_graphs.append(r)\n",
    "            # We must return a list of Graph objects\n",
    "        return all_graphs\n",
    "\n",
    "\n",
    "data = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('simple_graph_data_w_edges.pkl', 'wb') as b:\n",
    "    pickle.dump(data, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead going to seperate out the last few plays first and make that our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        all_graphs = []\n",
    "        indeces = edge_feat.index\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=40) as executor:\n",
    "            for r in executor.map(make_graph, indeces):\n",
    "                all_graphs.append(r)\n",
    "            # We must return a list of Graph objects\n",
    "        return all_graphs\n",
    "    \n",
    "data = pd.read_pickle('simple_graph_data_w_edges.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[150959:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:150959]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split\n",
    "idxs = np.random.permutation(len(data))\n",
    "split_va = int(0.9 * len(data))\n",
    "idx_tr, idx_va = np.split(idxs, [split_va])\n",
    "data_tr = data[idx_tr]\n",
    "data_va = data[idx_va]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01  # Learning rate\n",
    "epochs = 400  # Number of training epochs\n",
    "es_patience = 10  # Patience for early stopping\n",
    "batch_size = 32  # Batch size\n",
    "layers = 3 # Number of CrystalConv layers\n",
    "channels = 128 # Number of hidden nodes\n",
    "n_out = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = DisjointLoader(data_va, batch_size=batch_size)\n",
    "loader_te = DisjointLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(Model):\n",
    "    '''\n",
    "    Building the Graph Neural Network configuration with Model as the parent class \n",
    "    from spektral library.\n",
    "    '''\n",
    "    def __init__(self, n_layers):\n",
    "        '''\n",
    "        Constructor code for setting up the layers needed for training the model.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv1 = CrystalConv()\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                CrystalConv()\n",
    "            )\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation = tf.keras.layers.LeakyReLU(alpha = 0.1))\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(channels, activation = tf.keras.layers.LeakyReLU(alpha = 0.1))\n",
    "        self.dense3 = Dense(n_out, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        Build the neural network.\n",
    "        '''\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a, e])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense3(x)\n",
    "    \n",
    "model = GNN(layers)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        #print(target)\n",
    "        #print(inputs)\n",
    "        #print(pred)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can2hr/.local/lib/python3.9/site-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 - Loss: 1.709 - Acc: 0.338 - Val loss: 1.546 - Val acc: 0.379\n",
      "New best val_loss 1.546\n",
      "Ep. 2 - Loss: 1.590 - Acc: 0.353 - Val loss: 1.535 - Val acc: 0.384\n",
      "New best val_loss 1.535\n",
      "Ep. 3 - Loss: 1.615 - Acc: 0.348 - Val loss: 1.531 - Val acc: 0.387\n",
      "New best val_loss 1.531\n",
      "Ep. 4 - Loss: 1.511 - Acc: 0.395 - Val loss: 1.419 - Val acc: 0.437\n",
      "New best val_loss 1.419\n",
      "Ep. 5 - Loss: 1.510 - Acc: 0.399 - Val loss: 1.420 - Val acc: 0.423\n",
      "Ep. 6 - Loss: 1.438 - Acc: 0.429 - Val loss: 1.335 - Val acc: 0.470\n",
      "New best val_loss 1.335\n",
      "Ep. 7 - Loss: 1.471 - Acc: 0.437 - Val loss: 1.322 - Val acc: 0.477\n",
      "New best val_loss 1.322\n",
      "Ep. 8 - Loss: 1.538 - Acc: 0.435 - Val loss: 1.628 - Val acc: 0.329\n",
      "Ep. 9 - Loss: 1.446 - Acc: 0.422 - Val loss: 1.345 - Val acc: 0.463\n",
      "Ep. 10 - Loss: 1.376 - Acc: 0.454 - Val loss: 1.309 - Val acc: 0.490\n",
      "New best val_loss 1.309\n",
      "Ep. 11 - Loss: 1.370 - Acc: 0.463 - Val loss: 1.314 - Val acc: 0.483\n",
      "Ep. 12 - Loss: 1.677 - Acc: 0.404 - Val loss: 1.333 - Val acc: 0.489\n",
      "Ep. 13 - Loss: 1.414 - Acc: 0.445 - Val loss: 1.286 - Val acc: 0.502\n",
      "New best val_loss 1.286\n",
      "Ep. 14 - Loss: 1.366 - Acc: 0.460 - Val loss: 1.284 - Val acc: 0.498\n",
      "New best val_loss 1.284\n",
      "Ep. 15 - Loss: 1.454 - Acc: 0.445 - Val loss: 1.309 - Val acc: 0.466\n",
      "Ep. 16 - Loss: 1.357 - Acc: 0.464 - Val loss: 1.294 - Val acc: 0.485\n",
      "Ep. 17 - Loss: 1.369 - Acc: 0.460 - Val loss: 1.300 - Val acc: 0.484\n",
      "Ep. 18 - Loss: 1.356 - Acc: 0.467 - Val loss: 1.276 - Val acc: 0.496\n",
      "New best val_loss 1.276\n",
      "Ep. 19 - Loss: 1.566 - Acc: 0.435 - Val loss: 1.422 - Val acc: 0.442\n",
      "Ep. 20 - Loss: 1.397 - Acc: 0.454 - Val loss: 1.264 - Val acc: 0.488\n",
      "New best val_loss 1.264\n",
      "Ep. 21 - Loss: 1.365 - Acc: 0.465 - Val loss: 1.290 - Val acc: 0.488\n",
      "Ep. 22 - Loss: 1.341 - Acc: 0.474 - Val loss: 1.285 - Val acc: 0.494\n",
      "Ep. 23 - Loss: 1.323 - Acc: 0.479 - Val loss: 1.235 - Val acc: 0.512\n",
      "New best val_loss 1.235\n",
      "Ep. 24 - Loss: 1.325 - Acc: 0.480 - Val loss: 1.249 - Val acc: 0.514\n",
      "Ep. 25 - Loss: 1.321 - Acc: 0.482 - Val loss: 1.255 - Val acc: 0.502\n",
      "Ep. 26 - Loss: 1.317 - Acc: 0.484 - Val loss: 1.242 - Val acc: 0.511\n",
      "Ep. 27 - Loss: 1.318 - Acc: 0.485 - Val loss: 1.234 - Val acc: 0.513\n",
      "New best val_loss 1.234\n",
      "Ep. 28 - Loss: 1.308 - Acc: 0.486 - Val loss: 1.209 - Val acc: 0.529\n",
      "New best val_loss 1.209\n",
      "Ep. 29 - Loss: 1.308 - Acc: 0.487 - Val loss: 1.244 - Val acc: 0.510\n",
      "Ep. 30 - Loss: 1.307 - Acc: 0.489 - Val loss: 1.229 - Val acc: 0.509\n",
      "Ep. 31 - Loss: 1.306 - Acc: 0.490 - Val loss: 1.209 - Val acc: 0.524\n",
      "Ep. 32 - Loss: 1.309 - Acc: 0.489 - Val loss: 1.277 - Val acc: 0.493\n",
      "Ep. 33 - Loss: 1.304 - Acc: 0.492 - Val loss: 1.195 - Val acc: 0.540\n",
      "New best val_loss 1.195\n",
      "Ep. 34 - Loss: 1.302 - Acc: 0.493 - Val loss: 1.254 - Val acc: 0.514\n",
      "Ep. 35 - Loss: 1.300 - Acc: 0.494 - Val loss: 1.513 - Val acc: 0.432\n",
      "Ep. 36 - Loss: 1.308 - Acc: 0.494 - Val loss: 1.228 - Val acc: 0.526\n",
      "Ep. 37 - Loss: 1.309 - Acc: 0.492 - Val loss: 1.344 - Val acc: 0.465\n",
      "Ep. 38 - Loss: 1.303 - Acc: 0.494 - Val loss: 1.258 - Val acc: 0.501\n",
      "Ep. 39 - Loss: 1.295 - Acc: 0.497 - Val loss: 1.173 - Val acc: 0.552\n",
      "New best val_loss 1.173\n",
      "Ep. 40 - Loss: 1.296 - Acc: 0.499 - Val loss: 1.232 - Val acc: 0.512\n",
      "Ep. 41 - Loss: 1.333 - Acc: 0.486 - Val loss: 1.241 - Val acc: 0.515\n",
      "Ep. 42 - Loss: 1.296 - Acc: 0.499 - Val loss: 1.199 - Val acc: 0.532\n",
      "Ep. 43 - Loss: 1.352 - Acc: 0.483 - Val loss: 1.246 - Val acc: 0.508\n",
      "Ep. 44 - Loss: 1.292 - Acc: 0.498 - Val loss: 1.200 - Val acc: 0.536\n",
      "Ep. 45 - Loss: 1.289 - Acc: 0.500 - Val loss: 1.266 - Val acc: 0.512\n",
      "Ep. 46 - Loss: 1.321 - Acc: 0.490 - Val loss: 1.221 - Val acc: 0.536\n",
      "Ep. 47 - Loss: 1.932 - Acc: 0.475 - Val loss: 16.570 - Val acc: 0.441\n",
      "Ep. 48 - Loss: 2.661 - Acc: 0.412 - Val loss: 1.304 - Val acc: 0.488\n",
      "Ep. 49 - Loss: 1.404 - Acc: 0.452 - Val loss: 1.227 - Val acc: 0.512\n",
      "Early stopping (best val_loss: 1.1734064015119114)\n",
      "CPU times: user 31min 44s, sys: 2min 26s, total: 34min 11s\n",
      "Wall time: 31min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epoch = step = 0\n",
    "best_val_loss = np.inf\n",
    "best_weights = None\n",
    "patience = es_patience\n",
    "results = []\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss, acc = train_step(*batch)\n",
    "    results.append((loss, acc))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss, val_acc = evaluate(loader_va)\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check if loss improved for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = es_patience\n",
    "            print(\"New best val_loss {:.3f}\".format(val_loss))\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
    "                break\n",
    "        results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/simple_best_model', compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 1.2245. Test acc: 0.54\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(best_weights)  # Load best model\n",
    "test_loss, test_acc = evaluate(loader_te)\n",
    "print(\"Done. Test loss: {:.4f}. Test acc: {:.2f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_te = DisjointLoader(test_data, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 1.1955. Test acc: 0.55\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(loader_te)\n",
    "print(\"Done. Test loss: {:.4f}. Test acc: {:.2f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_play(loader):\n",
    "    true = []\n",
    "    predict = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        true.append(target)\n",
    "        predict.append(pred)\n",
    "    return true, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "true, predict = evaluate_play(loader_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "for batch in predict:\n",
    "    for i in batch:\n",
    "        pred = pd.DataFrame(i).T\n",
    "        predictions = pd.concat([predictions, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['predicted_coverage'] = predictions.idxmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = pd.DataFrame(y[150959:].idxmax(axis=1)).rename(columns = {0:'coverage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage['predicted_coverage'] = predictions['predicted_coverage'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5508479067302596"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(coverage['predicted_coverage'] == coverage['coverage'])/len(coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should be 55% and expected to be higher once we look by play at the most predicted one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = coverage.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueplayId</th>\n",
       "      <th>frameId</th>\n",
       "      <th>coverage</th>\n",
       "      <th>predicted_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20211017043813</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20211017043813</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20211017043813</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20211017043813</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20211017043813</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37735</th>\n",
       "      <td>2021102500933</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37736</th>\n",
       "      <td>2021102500933</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37737</th>\n",
       "      <td>2021102500933</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37738</th>\n",
       "      <td>2021102500933</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37739</th>\n",
       "      <td>2021102500933</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37740 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         uniqueplayId  frameId  coverage  predicted_coverage\n",
       "0      20211017043813      6.0         3                   3\n",
       "1      20211017043813      7.0         3                   3\n",
       "2      20211017043813      8.0         3                   3\n",
       "3      20211017043813      9.0         3                   3\n",
       "4      20211017043813     10.0         3                   3\n",
       "...               ...      ...       ...                 ...\n",
       "37735   2021102500933     34.0         4                   3\n",
       "37736   2021102500933     35.0         4                   4\n",
       "37737   2021102500933     36.0         4                   4\n",
       "37738   2021102500933     37.0         4                   3\n",
       "37739   2021102500933     38.0         4                   3\n",
       "\n",
       "[37740 rows x 4 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(coverage.groupby(['uniqueplayId','predicted_coverage']).size()).rename(columns = {0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.loc[counts.groupby(['uniqueplayId'])[\"count\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(coverage.groupby('uniqueplayId').first()['coverage']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.merge(counts, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueplayId</th>\n",
       "      <th>coverage</th>\n",
       "      <th>predicted_coverage</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202110170483</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202110170673</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202110170762</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202110170856</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202110170955</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>20211025003684</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>20211025003735</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>20211025003904</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>20211025003926</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>20211025003945</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueplayId  coverage  predicted_coverage  count\n",
       "0       202110170483         3                   3     15\n",
       "1       202110170673         3                   4     15\n",
       "2       202110170762         1                   1     22\n",
       "3       202110170856         3                   3     18\n",
       "4       202110170955         6                   6      8\n",
       "...              ...       ...                 ...    ...\n",
       "1415  20211025003684         1                   1     31\n",
       "1416  20211025003735         3                   1     17\n",
       "1417  20211025003904         4                   4     23\n",
       "1418  20211025003926         2                   6     15\n",
       "1419  20211025003945         5                   5     14\n",
       "\n",
       "[1420 rows x 4 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5823943661971831"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(results['predicted_coverage'] == results['coverage'])/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall 58% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.8.0/Keras Py3.9",
   "language": "python",
   "name": "tensorflow-2.8.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
