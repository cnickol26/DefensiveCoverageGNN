{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from multiprocessing import Process\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GlobalAvgPool, ECCConv\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "from spektral.utils import reorder\n",
    "from ipywidgets import Checkbox, Dropdown, Accordion, VBox\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from spektral.data import Dataset, Graph, DisjointLoader\n",
    "from spektral.layers import CrystalConv, GlobalAvgPool\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from os.path import isfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = pd.read_pickle('simple_nodes.pkl')\n",
    "adj_mat = pd.read_pickle('adj_mat.pkl')\n",
    "edge_feat = pd.read_pickle('edge_features.pkl')\n",
    "y = pd.read_pickle('y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feat = adj_mat * edge_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reset_index()\n",
    "y['uniqueplayId'] = y['uniqueplayId'].astype(int)\n",
    "y = y.set_index(['uniqueplayId','frameId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_x                0\n",
       "new_y                0\n",
       "Defense              0\n",
       "o                    0\n",
       "score_d              0\n",
       "frames_after_snap    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feat = node_feat.reset_index()\n",
    "node_feat['uniqueplayId'] = node_feat['uniqueplayId'].astype(int)\n",
    "node_feat = node_feat.set_index(['uniqueplayId','frameId','nflId'])\n",
    "node_feat.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(index):\n",
    "    ## PLAY_ID MUST BE A STRING\n",
    "    \n",
    "    play_id = index[0]\n",
    "    frame_id = index[1]\n",
    "\n",
    "    # Node features\n",
    "    ## filter the node features matrix by given play id and frame id\n",
    "    x_temp = node_feat.query(f'(uniqueplayId=={play_id})&(frameId=={frame_id})')\n",
    "    x_temp = np.array(x_temp)\n",
    "    #print(x_temp.shape)\n",
    "\n",
    "    # Adjacency\n",
    "    a_temp = adj_mat[(play_id, frame_id)]\n",
    "    a_temp = sp.csr_matrix(a_temp)\n",
    "    #print(a_temp.shape)\n",
    "    \n",
    "    # Edges\n",
    "    ## get the correct edge matrix based on play id and frame id\n",
    "    e_temp = edge_feat[(play_id, frame_id)]\n",
    "    e_sp_mat = sp.find(e_temp)\n",
    "\n",
    "    edge_indeces = np.array([e_sp_mat[0], e_sp_mat[1]]).T\n",
    "\n",
    "    edge_vals = e_sp_mat[2]\n",
    "    e_temp = reorder(edge_indeces, edge_features=edge_vals)[1].reshape(len(edge_vals), 1)\n",
    "    #print(e_temp.shape)\n",
    "\n",
    "    # Labels\n",
    "    ## get the single label of coverage from y for that play id\n",
    "    y_temp = y.query(f'(uniqueplayId=={play_id})&(frameId=={frame_id})').values[0]\n",
    "    #print(y_temp.shape)\n",
    "\n",
    "    return Graph(x=x_temp, a=a_temp, e=e_temp, y=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=23, n_node_features=6, n_edge_features=1, n_labels=7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_graph(('202109090097', 6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 49.2 s, total: 3min 46s\n",
      "Wall time: 36min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "# Load data\n",
    "################################################################################\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        all_graphs = []\n",
    "        indeces = adj_mat.index\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=40) as executor:\n",
    "            for r in executor.map(make_graph, indeces):\n",
    "                all_graphs.append(r)\n",
    "            # We must return a list of Graph objects\n",
    "        return all_graphs\n",
    "\n",
    "\n",
    "data = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('complex_graphs.pkl', 'wb') as b:\n",
    "    pickle.dump(data, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        all_graphs = []\n",
    "        indeces = edge_feat.index\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=40) as executor:\n",
    "            for r in executor.map(make_graph, indeces):\n",
    "                all_graphs.append(r)\n",
    "            # We must return a list of Graph objects\n",
    "        return all_graphs\n",
    "    \n",
    "data = pd.read_pickle('complex_graphs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[150959:]\n",
    "data = data[:150959]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split\n",
    "idxs = np.random.permutation(len(data))\n",
    "split_va = int(0.9 * len(data))\n",
    "idx_tr, idx_va = np.split(idxs, [split_va])\n",
    "data_tr = data[idx_tr]\n",
    "data_va = data[idx_va]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01  # Learning rate\n",
    "epochs = 400  # Number of training epochs\n",
    "es_patience = 25  # Patience for early stopping\n",
    "batch_size = 32  # Batch size\n",
    "layers = 3 # Number of CrystalConv layers\n",
    "channels = 128 # Number of hidden nodes\n",
    "n_out = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = DisjointLoader(data_va, batch_size=batch_size)\n",
    "loader_te = DisjointLoader(test_data, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(Model):\n",
    "    '''\n",
    "    Building the Graph Neural Network configuration with Model as the parent class \n",
    "    from spektral library.\n",
    "    '''\n",
    "    def __init__(self, n_layers):\n",
    "        '''\n",
    "        Constructor code for setting up the layers needed for training the model.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv1 = CrystalConv()\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                CrystalConv()\n",
    "            )\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation = tf.keras.layers.LeakyReLU(alpha = 0.1))\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(channels, activation = tf.keras.layers.LeakyReLU(alpha = 0.1))\n",
    "        self.dense3 = Dense(n_out, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        Build the neural network.\n",
    "        '''\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a, e])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense3(x)\n",
    "    \n",
    "model = GNN(layers)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        #print(target)\n",
    "        #print(inputs)\n",
    "        #print(pred)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can2hr/.local/lib/python3.8/site-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n",
      "/home/can2hr/.local/lib/python3.8/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 - Loss: 1.873 - Acc: 0.318 - Val loss: 1.637 - Val acc: 0.342\n",
      "New best val_loss 1.637\n",
      "Ep. 2 - Loss: 1.648 - Acc: 0.341 - Val loss: 1.581 - Val acc: 0.364\n",
      "New best val_loss 1.581\n",
      "Ep. 3 - Loss: 1.625 - Acc: 0.351 - Val loss: 1.592 - Val acc: 0.368\n",
      "Ep. 4 - Loss: 1.647 - Acc: 0.363 - Val loss: 1.866 - Val acc: 0.153\n",
      "Ep. 5 - Loss: 1.787 - Acc: 0.321 - Val loss: 1.660 - Val acc: 0.328\n",
      "Ep. 6 - Loss: 1.642 - Acc: 0.338 - Val loss: 1.533 - Val acc: 0.397\n",
      "New best val_loss 1.533\n",
      "Ep. 7 - Loss: 2.102 - Acc: 0.324 - Val loss: 1.701 - Val acc: 0.338\n",
      "Ep. 8 - Loss: 1.639 - Acc: 0.358 - Val loss: 1.491 - Val acc: 0.419\n",
      "New best val_loss 1.491\n",
      "Ep. 9 - Loss: 1.489 - Acc: 0.401 - Val loss: 1.376 - Val acc: 0.422\n",
      "New best val_loss 1.376\n",
      "Ep. 10 - Loss: 1.422 - Acc: 0.425 - Val loss: 1.331 - Val acc: 0.483\n",
      "New best val_loss 1.331\n",
      "Ep. 11 - Loss: 1.410 - Acc: 0.434 - Val loss: 1.266 - Val acc: 0.491\n",
      "New best val_loss 1.266\n",
      "Ep. 12 - Loss: 1.401 - Acc: 0.447 - Val loss: 1.349 - Val acc: 0.476\n",
      "Ep. 13 - Loss: 1.730 - Acc: 0.412 - Val loss: 1.373 - Val acc: 0.424\n",
      "Ep. 14 - Loss: 1.384 - Acc: 0.449 - Val loss: 1.296 - Val acc: 0.493\n",
      "Ep. 15 - Loss: 1.317 - Acc: 0.487 - Val loss: 1.231 - Val acc: 0.523\n",
      "New best val_loss 1.231\n",
      "Ep. 16 - Loss: 1.324 - Acc: 0.492 - Val loss: 1.235 - Val acc: 0.522\n",
      "Ep. 17 - Loss: 1.292 - Acc: 0.502 - Val loss: 1.204 - Val acc: 0.539\n",
      "New best val_loss 1.204\n",
      "Ep. 18 - Loss: 1.296 - Acc: 0.502 - Val loss: 1.225 - Val acc: 0.517\n",
      "Ep. 19 - Loss: 1.319 - Acc: 0.492 - Val loss: 1.281 - Val acc: 0.502\n",
      "Ep. 20 - Loss: 1.269 - Acc: 0.512 - Val loss: 1.179 - Val acc: 0.558\n",
      "New best val_loss 1.179\n",
      "Ep. 21 - Loss: 1.399 - Acc: 0.474 - Val loss: 1.231 - Val acc: 0.540\n",
      "Ep. 22 - Loss: 1.263 - Acc: 0.515 - Val loss: 1.171 - Val acc: 0.543\n",
      "New best val_loss 1.171\n",
      "Ep. 23 - Loss: 1.321 - Acc: 0.496 - Val loss: 1.287 - Val acc: 0.468\n",
      "Ep. 24 - Loss: 1.269 - Acc: 0.510 - Val loss: 1.179 - Val acc: 0.550\n",
      "Ep. 25 - Loss: 1.827 - Acc: 0.450 - Val loss: 1.229 - Val acc: 0.521\n",
      "Ep. 26 - Loss: 1.274 - Acc: 0.510 - Val loss: 1.166 - Val acc: 0.555\n",
      "New best val_loss 1.166\n",
      "Ep. 27 - Loss: 1.240 - Acc: 0.526 - Val loss: 1.211 - Val acc: 0.514\n",
      "Ep. 28 - Loss: 1.266 - Acc: 0.519 - Val loss: 1.163 - Val acc: 0.554\n",
      "New best val_loss 1.163\n",
      "Ep. 29 - Loss: 1.694 - Acc: 0.487 - Val loss: 1.585 - Val acc: 0.376\n",
      "Ep. 30 - Loss: 1.463 - Acc: 0.424 - Val loss: 1.248 - Val acc: 0.511\n",
      "Ep. 31 - Loss: 1.253 - Acc: 0.515 - Val loss: 1.150 - Val acc: 0.559\n",
      "New best val_loss 1.150\n",
      "Ep. 32 - Loss: 1.641 - Acc: 0.471 - Val loss: 1.628 - Val acc: 0.342\n",
      "Ep. 33 - Loss: 1.456 - Acc: 0.424 - Val loss: 1.233 - Val acc: 0.506\n",
      "Ep. 34 - Loss: 1.294 - Acc: 0.499 - Val loss: 1.135 - Val acc: 0.563\n",
      "New best val_loss 1.135\n",
      "Ep. 35 - Loss: 2.879 - Acc: 0.447 - Val loss: 1.937 - Val acc: 0.306\n",
      "Ep. 36 - Loss: 2.140 - Acc: 0.284 - Val loss: 1.624 - Val acc: 0.368\n",
      "Ep. 37 - Loss: 1.650 - Acc: 0.357 - Val loss: 1.443 - Val acc: 0.435\n",
      "Ep. 38 - Loss: 1.616 - Acc: 0.377 - Val loss: 1.391 - Val acc: 0.439\n",
      "Ep. 39 - Loss: 1.368 - Acc: 0.461 - Val loss: 1.231 - Val acc: 0.517\n",
      "Ep. 40 - Loss: 1.547 - Acc: 0.454 - Val loss: 1.390 - Val acc: 0.454\n",
      "Ep. 41 - Loss: 1.347 - Acc: 0.472 - Val loss: 1.202 - Val acc: 0.535\n",
      "Ep. 42 - Loss: 1.276 - Acc: 0.506 - Val loss: 1.158 - Val acc: 0.546\n",
      "Ep. 43 - Loss: 3.255 - Acc: 0.440 - Val loss: 1.326 - Val acc: 0.479\n",
      "Ep. 44 - Loss: 1.315 - Acc: 0.493 - Val loss: 1.164 - Val acc: 0.548\n",
      "Ep. 45 - Loss: 1.279 - Acc: 0.511 - Val loss: 1.137 - Val acc: 0.549\n",
      "Ep. 46 - Loss: 1.283 - Acc: 0.511 - Val loss: 1.241 - Val acc: 0.521\n",
      "Ep. 47 - Loss: 1.275 - Acc: 0.515 - Val loss: 1.205 - Val acc: 0.516\n",
      "Ep. 48 - Loss: 1.332 - Acc: 0.499 - Val loss: 1.145 - Val acc: 0.570\n",
      "Ep. 49 - Loss: 1.251 - Acc: 0.522 - Val loss: 1.172 - Val acc: 0.560\n",
      "Ep. 50 - Loss: 1.220 - Acc: 0.532 - Val loss: 1.126 - Val acc: 0.573\n",
      "New best val_loss 1.126\n",
      "Ep. 51 - Loss: 7.107 - Acc: 0.309 - Val loss: 1.742 - Val acc: 0.324\n",
      "Ep. 52 - Loss: 4.307 - Acc: 0.283 - Val loss: 1.695 - Val acc: 0.325\n",
      "Ep. 53 - Loss: 1.764 - Acc: 0.303 - Val loss: 1.594 - Val acc: 0.377\n",
      "Ep. 54 - Loss: 2.023 - Acc: 0.315 - Val loss: 1.556 - Val acc: 0.382\n",
      "Ep. 55 - Loss: 1.542 - Acc: 0.396 - Val loss: 1.417 - Val acc: 0.453\n",
      "Ep. 56 - Loss: 1.495 - Acc: 0.453 - Val loss: 3.155 - Val acc: 0.270\n",
      "Ep. 57 - Loss: 2.356 - Acc: 0.330 - Val loss: 1.447 - Val acc: 0.432\n",
      "Ep. 58 - Loss: 1.429 - Acc: 0.447 - Val loss: 1.379 - Val acc: 0.512\n",
      "Ep. 59 - Loss: 1.308 - Acc: 0.494 - Val loss: 1.210 - Val acc: 0.530\n",
      "Ep. 60 - Loss: 1.724 - Acc: 0.425 - Val loss: 1.266 - Val acc: 0.500\n",
      "Ep. 61 - Loss: 1.300 - Acc: 0.494 - Val loss: 1.187 - Val acc: 0.541\n",
      "Ep. 62 - Loss: 2.000 - Acc: 0.455 - Val loss: 1.381 - Val acc: 0.456\n",
      "Ep. 63 - Loss: 1.364 - Acc: 0.461 - Val loss: 1.238 - Val acc: 0.532\n",
      "Ep. 64 - Loss: 1.280 - Acc: 0.506 - Val loss: 1.169 - Val acc: 0.545\n",
      "Ep. 65 - Loss: 1.529 - Acc: 0.474 - Val loss: 1.209 - Val acc: 0.512\n",
      "Ep. 66 - Loss: 1.268 - Acc: 0.511 - Val loss: 1.168 - Val acc: 0.555\n",
      "Ep. 67 - Loss: 1.463 - Acc: 0.470 - Val loss: 1.211 - Val acc: 0.538\n",
      "Ep. 68 - Loss: 1.288 - Acc: 0.508 - Val loss: 1.169 - Val acc: 0.550\n",
      "Ep. 69 - Loss: 1.258 - Acc: 0.521 - Val loss: 1.177 - Val acc: 0.546\n",
      "Ep. 70 - Loss: 1.547 - Acc: 0.464 - Val loss: 1.229 - Val acc: 0.516\n",
      "Ep. 71 - Loss: 1.253 - Acc: 0.519 - Val loss: 1.124 - Val acc: 0.569\n",
      "New best val_loss 1.124\n",
      "Ep. 72 - Loss: 9.140 - Acc: 0.357 - Val loss: 1.572 - Val acc: 0.362\n",
      "Ep. 73 - Loss: 1.572 - Acc: 0.365 - Val loss: 1.378 - Val acc: 0.437\n",
      "Ep. 74 - Loss: 1.547 - Acc: 0.437 - Val loss: 1.291 - Val acc: 0.514\n",
      "Ep. 75 - Loss: 1.307 - Acc: 0.497 - Val loss: 1.236 - Val acc: 0.510\n",
      "Ep. 76 - Loss: 1.725 - Acc: 0.493 - Val loss: 1.666 - Val acc: 0.369\n",
      "Ep. 77 - Loss: 1.414 - Acc: 0.455 - Val loss: 1.201 - Val acc: 0.549\n",
      "Ep. 78 - Loss: 1.517 - Acc: 0.483 - Val loss: 1.565 - Val acc: 0.394\n",
      "Ep. 79 - Loss: 1.342 - Acc: 0.487 - Val loss: 1.184 - Val acc: 0.535\n",
      "Ep. 80 - Loss: 1.368 - Acc: 0.503 - Val loss: 1.589 - Val acc: 0.435\n",
      "Ep. 81 - Loss: 21.472 - Acc: 0.288 - Val loss: 1.656 - Val acc: 0.352\n",
      "Ep. 82 - Loss: 1.745 - Acc: 0.339 - Val loss: 1.532 - Val acc: 0.398\n",
      "Ep. 83 - Loss: 1.568 - Acc: 0.397 - Val loss: 1.369 - Val acc: 0.477\n",
      "Ep. 84 - Loss: 1.392 - Acc: 0.464 - Val loss: 1.305 - Val acc: 0.487\n",
      "Ep. 85 - Loss: 1.289 - Acc: 0.508 - Val loss: 1.194 - Val acc: 0.547\n",
      "Ep. 86 - Loss: 12.708 - Acc: 0.343 - Val loss: 1.620 - Val acc: 0.381\n",
      "Ep. 87 - Loss: 1.682 - Acc: 0.359 - Val loss: 1.459 - Val acc: 0.423\n",
      "Ep. 88 - Loss: 1.466 - Acc: 0.437 - Val loss: 1.291 - Val acc: 0.491\n",
      "Ep. 89 - Loss: 1.342 - Acc: 0.484 - Val loss: 2.149 - Val acc: 0.531\n",
      "Ep. 90 - Loss: 1.622 - Acc: 0.434 - Val loss: 1.382 - Val acc: 0.445\n",
      "Ep. 91 - Loss: 1.516 - Acc: 0.441 - Val loss: 1.221 - Val acc: 0.519\n",
      "Ep. 92 - Loss: 22.508 - Acc: 0.438 - Val loss: 1.790 - Val acc: 0.330\n",
      "Ep. 93 - Loss: 1.935 - Acc: 0.296 - Val loss: 1.570 - Val acc: 0.370\n",
      "Ep. 94 - Loss: 1.577 - Acc: 0.390 - Val loss: 1.335 - Val acc: 0.472\n",
      "Ep. 95 - Loss: 1.480 - Acc: 0.457 - Val loss: 1.401 - Val acc: 0.444\n",
      "Ep. 96 - Loss: 9.841 - Acc: 0.281 - Val loss: 1.639 - Val acc: 0.330\n",
      "Early stopping (best val_loss: 1.1238406427840297)\n",
      "CPU times: user 3h 29min 53s, sys: 15min, total: 3h 44min 53s\n",
      "Wall time: 1h 30min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epoch = step = 0\n",
    "best_val_loss = np.inf\n",
    "best_weights = None\n",
    "patience = es_patience\n",
    "results = []\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss, acc = train_step(*batch)\n",
    "    results.append((loss, acc))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss, val_acc = evaluate(loader_va)\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check if loss improved for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = es_patience\n",
    "            print(\"New best val_loss {:.3f}\".format(val_loss))\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
    "                break\n",
    "        results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_5 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, leaky_re_lu_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/complex_best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/complex_best_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(best_weights) \n",
    "model.save('saved_model/complex_best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can2hr/.local/lib/python3.8/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('saved_model/complex_best_model', compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 1.1517. Test acc: 0.57\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(loader_te)\n",
    "print(\"Done. Test loss: {:.4f}. Test acc: {:.2f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_play(loader):\n",
    "    true = []\n",
    "    predict = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        true.append(target)\n",
    "        predict.append(pred)\n",
    "    return true, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "true, predict = evaluate_play(loader_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "for batch in predict:\n",
    "    for i in batch:\n",
    "        pred = pd.DataFrame(i).T\n",
    "        predictions = pd.concat([predictions, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5677795442501324"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['predicted_coverage'] = predictions.idxmax(axis = 1)\n",
    "coverage = pd.DataFrame(y[150959:].idxmax(axis=1)).rename(columns = {0:'coverage'})\n",
    "coverage['predicted_coverage'] = predictions['predicted_coverage'].values\n",
    "sum(coverage['predicted_coverage'] == coverage['coverage'])/len(coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = coverage.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(coverage.groupby(['uniqueplayId','predicted_coverage']).size()).rename(columns = {0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.loc[counts.groupby(['uniqueplayId'])[\"count\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(coverage.groupby('uniqueplayId').first()['coverage']).reset_index()\n",
    "results = results.merge(counts, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6035211267605634"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(results['predicted_coverage'] == results['coverage'])/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate accuracy calculation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = pd.DataFrame(y[150959:].idxmax(axis=1)).rename(columns = {0:'coverage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.039442</td>\n",
       "      <td>0.491626</td>\n",
       "      <td>0.030557</td>\n",
       "      <td>0.099340</td>\n",
       "      <td>0.080536</td>\n",
       "      <td>0.256220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.041547</td>\n",
       "      <td>0.460751</td>\n",
       "      <td>0.037469</td>\n",
       "      <td>0.121662</td>\n",
       "      <td>0.068214</td>\n",
       "      <td>0.267506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.425344</td>\n",
       "      <td>0.047913</td>\n",
       "      <td>0.152193</td>\n",
       "      <td>0.053128</td>\n",
       "      <td>0.274963</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.032414</td>\n",
       "      <td>0.544076</td>\n",
       "      <td>0.016310</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.119064</td>\n",
       "      <td>0.228092</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.033154</td>\n",
       "      <td>0.518805</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.102649</td>\n",
       "      <td>0.249262</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041752</td>\n",
       "      <td>0.066558</td>\n",
       "      <td>0.279795</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.017510</td>\n",
       "      <td>0.545954</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>37735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060969</td>\n",
       "      <td>0.114076</td>\n",
       "      <td>0.270709</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.024884</td>\n",
       "      <td>0.472626</td>\n",
       "      <td>0.055720</td>\n",
       "      <td>37736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046937</td>\n",
       "      <td>0.061138</td>\n",
       "      <td>0.369192</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.045648</td>\n",
       "      <td>0.386835</td>\n",
       "      <td>0.089064</td>\n",
       "      <td>37737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038622</td>\n",
       "      <td>0.048735</td>\n",
       "      <td>0.374062</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.043594</td>\n",
       "      <td>0.406255</td>\n",
       "      <td>0.087929</td>\n",
       "      <td>37738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010602</td>\n",
       "      <td>0.046038</td>\n",
       "      <td>0.343220</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.007912</td>\n",
       "      <td>0.548068</td>\n",
       "      <td>0.043819</td>\n",
       "      <td>37739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37740 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.002279  0.039442  0.491626  0.030557  0.099340  0.080536  0.256220   \n",
       "0   0.002851  0.041547  0.460751  0.037469  0.121662  0.068214  0.267506   \n",
       "0   0.003382  0.043076  0.425344  0.047913  0.152193  0.053128  0.274963   \n",
       "0   0.001650  0.032414  0.544076  0.016310  0.058394  0.119064  0.228092   \n",
       "0   0.002097  0.033154  0.518805  0.019712  0.074321  0.102649  0.249262   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "0   0.041752  0.066558  0.279795  0.000385  0.017510  0.545954  0.048046   \n",
       "0   0.060969  0.114076  0.270709  0.001016  0.024884  0.472626  0.055720   \n",
       "0   0.046937  0.061138  0.369192  0.001186  0.045648  0.386835  0.089064   \n",
       "0   0.038622  0.048735  0.374062  0.000804  0.043594  0.406255  0.087929   \n",
       "0   0.010602  0.046038  0.343220  0.000342  0.007912  0.548068  0.043819   \n",
       "\n",
       "     temp  \n",
       "0       0  \n",
       "0       1  \n",
       "0       2  \n",
       "0       3  \n",
       "0       4  \n",
       "..    ...  \n",
       "0   37735  \n",
       "0   37736  \n",
       "0   37737  \n",
       "0   37738  \n",
       "0   37739  \n",
       "\n",
       "[37740 rows x 8 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage['temp'] = np.arange(len(coverage))\n",
    "predictions['temp'] = np.arange(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = coverage.reset_index().merge(predictions, how = 'left').set_index(['uniqueplayId','frameId','coverage']).drop('temp', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = coverage.groupby('uniqueplayId').apply(lambda x: x.sum()/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueplayId</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202110170483</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202110170673</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202110170762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202110170856</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202110170955</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>20211025003684</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>20211025003735</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>20211025003904</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>20211025003926</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>20211025003945</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueplayId  prediction\n",
       "0       202110170483           3\n",
       "1       202110170673           3\n",
       "2       202110170762           1\n",
       "3       202110170856           3\n",
       "4       202110170955           4\n",
       "...              ...         ...\n",
       "1415  20211025003684           2\n",
       "1416  20211025003735           3\n",
       "1417  20211025003904           4\n",
       "1418  20211025003926           2\n",
       "1419  20211025003945           2\n",
       "\n",
       "[1420 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = pd.DataFrame(probs.idxmax(axis = 1)).rename(columns = {0:'prediction'}).reset_index()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = pd.DataFrame(y[150959:].idxmax(axis=1)).rename(columns = {0:'coverage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(plays.groupby('uniqueplayId').first()).reset_index()\n",
    "results = results.merge(probs, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueplayId</th>\n",
       "      <th>coverage</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202110170483</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202110170673</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202110170762</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202110170856</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202110170955</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>20211025003684</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>20211025003735</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>20211025003904</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>20211025003926</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>20211025003945</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueplayId  coverage  prediction\n",
       "0       202110170483         3           3\n",
       "1       202110170673         3           3\n",
       "2       202110170762         1           1\n",
       "3       202110170856         3           3\n",
       "4       202110170955         6           4\n",
       "...              ...       ...         ...\n",
       "1415  20211025003684         1           2\n",
       "1416  20211025003735         3           3\n",
       "1417  20211025003904         4           4\n",
       "1418  20211025003926         2           2\n",
       "1419  20211025003945         5           2\n",
       "\n",
       "[1420 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['values'] = str('coverage') + str('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueplayId</th>\n",
       "      <th>coverage</th>\n",
       "      <th>prediction</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202110170483</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202110170673</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202110170762</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202110170856</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202110170955</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>20211025003684</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>20211025003735</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>20211025003904</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>20211025003926</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>20211025003945</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>coverageprediction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueplayId  coverage  prediction              values\n",
       "0       202110170483         3           3  coverageprediction\n",
       "1       202110170673         3           3  coverageprediction\n",
       "2       202110170762         1           1  coverageprediction\n",
       "3       202110170856         3           3  coverageprediction\n",
       "4       202110170955         6           4  coverageprediction\n",
       "...              ...       ...         ...                 ...\n",
       "1415  20211025003684         1           2  coverageprediction\n",
       "1416  20211025003735         3           3  coverageprediction\n",
       "1417  20211025003904         4           4  coverageprediction\n",
       "1418  20211025003926         2           2  coverageprediction\n",
       "1419  20211025003945         5           2  coverageprediction\n",
       "\n",
       "[1420 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6330985915492958"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(results['prediction'] == results['coverage'])/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking performance just looking at the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = pd.DataFrame(y[150959:].idxmax(axis=1)).rename(columns = {0:'coverage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage['temp'] = np.arange(len(coverage))\n",
    "predictions['temp'] = np.arange(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = coverage.reset_index().merge(predictions, how = 'left').set_index(['uniqueplayId','frameId']).drop('temp', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = pd.DataFrame(coverage.groupby('uniqueplayId').first().reset_index().set_index(['uniqueplayId','coverage']).idxmax(axis=1)).rename(columns = {0:'prediction'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46056338028169014"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bayes['prediction'] == bayes['coverage'])/len(bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
